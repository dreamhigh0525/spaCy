include ../../header.jade
include ./meta.jade


+WritePost(Meta)
    section.intro
        p This document explains how to add languages to spaCy. It will be updated and improved as languages are added. For now, focus is on adding a new tokenizer. Further resources require annotated and unannotated data. For annotated data, we will select and license appropriate treebanks. For unannotated data, we will default to using Wikipedia. The running example will be German.
        
    details(open=true)
        summary: h4 Setup

        p Create the directories that the language data and the model class reside in. 

        pre.language-bash: code
            | $ mkdir spacy/de
            | $ mkdir lang_data/de
            | $ mkdir corpora/de

        p Write an initial #[code spacy/de/__init__.py] file.

        pre.language-python
            code
                | from __future__ import unicode_literals, print_function
                | from os import path
                | from ..language import Language
                | 
                | class German(Language):
                |     @classmethod
                |     def default_data_dir(cls):
                |         return path.join(path.dirname(__file__), 'data')

    details(open=true)
        summary: h4 Tokenizer

        p spaCy's tokenizer first collects a "chunk" of characters. To allow alignment, whitespace characters are preserved, but in separate chunks. However, chunks which are simply the character ' ' are suppressed. For example:

        table
            tr
                td 'One two, "three"'
                td ['One', 'two,', '"three"']
            tr
                td 'One  three four'
                td ['One', ' ', 'three', 'four']
            tr
                td ' \t\n two three'
                td [' \t\n ', 'two', 'three']
    
        p The chunks are then tokenized further, using prefix, suffix and infix expressions, with special-cases specified separately, to handle contractions, fused tokens, and tokens with protected punctuation or affix characters. Specifying these special-cases separately greatly simplifies the rules.


        details
            summary: h4 lang_data/de/prefix.txt
            
            pre
                | ,
                | "
                | (
                | [
                | {
                | *
                | <
                | $
                | £
                | “
                | '
                | ``
                | `
                | #
                | ‘
                | ....
                | ...
        details
            summary: h4 lang_data/de/suffix.txt

            pre
                | )
                | ]
                | }
                | "
                | ;
                | .
                | ,

        details
            summary: h4 lang_data/de/infix.txt

            pre
                | \.\.\.
                | (?<=[a-z])\.(?=[A-Z])
                | (?<=[a-zA-Z])-(?=[a-zA-z])

        details
            summary: h4 specials.json
        
            p A lookup-table is used to handle contractions and fused tokens.  The lookup table is specified in a json file. Each entry is keyed by the string. Its value is a list of token specifications, including the form, lemma, part-of-speech, and morphological features.

        details
            summary: h4 lemma_rules.json

            p Write lemmatization rules, and a list of exceptions. The English lemmatization rules can be seen #[a(href="https://github.com/honnibal/spaCy/blob/master/lang_data/en/lemma_rules.json") here], used by the lemmatizer #[a(href="https://github.com/honnibal/spaCy/blob/master/spacy/lemmatizer.py") here].
  
        details
            summary: h4 tag_map.json

            p Write a #[a(href="https://github.com/honnibal/spaCy/blob/master/lang_data/en/tag_map.json") tag_map.json] file, which maps the language-specific treebank tag scheme to the #[a(href="http://universaldependencies.github.io/docs/") universal part-of-speech scheme], with additional morphological features. 

        details
            summary: h4 morphs.json

            p Write a #[a(href="https://github.com/honnibal/spaCy/blob/master/lang_data/en/morphs.json") morphs.json] file, which lists special-cases for the morphological analyzer and lemmatizer. This file is keyed by the part-of-speech tag, and then by a list of orthographic forms. This allows words which require different morphological features, depending on their part-of-speech tag, to receive them.

    details
        summary: h4 Prepare Wikipedia dump

        p See #[a(href="https://github.com/wikilinks/wikijson") here].

        
    details: summary: h4 Create frequencies
        pre.language-bash: code
            | $ python bin/get_freqs.py
            | $ python bin/gather_freqs.py

    details: summary: h4 Brown clusters

    p See #[a(href="https://github.com/percyliang/brown-cluster") here].

    details: summary: h4 Over-write attribute functions if necessary

    details: summary: h4 Run init_model.py

    details: summary: h4 Train part-of-speech tagger

    details: summary: h4 Train dependency parser

    details: summary: h4 Train entity recogniser

