mixin declare_class(name, ref)
    details
        summary
            a(name=ref)
                span.declaration
                    span.label class
                    code #{name}
        block

mixin method(name, parameters, link_name)
    details(open=attributes.open)
        summary
            a(name=link_name)
                span.declaration
                    code= name
                    span.parameters
                        | #{parameters}
        block

mixin params
    ul
        block

mixin param(name, type, value)
    li
        if type
            <strong>#{name}</strong> (!{type}) &#8211;
        else
           <strong>#{name}</strong> &#8211;
        block

mixin attribute(name, type, value)
    details(open=attributes.open)
        summary
            span.declaration
                code= name
        p
            block

mixin returns(name, type, value)
    li
        if type
            <strong>#{name}</strong> (!{type}) &#8211;
        else
            <strong>#{name}</strong> &#8211;
        block

mixin returns(type)
    | tmp

mixin init
    details
        summary: h4 Constructors

        block

mixin callable
    details
        summary: h4 Callable

        block

mixin sequence
    details
        summary: h4 Sequence API

        block

mixin maptype
    details
        summary: h4 Map

        block

mixin summary
    block

mixin en_example
    pre.language-python
        code
            | from spacy.en import English
            | from spacy._doc_examples import download_war_and_peace
            | 
            | unprocessed_unicode = download_war_and_peace()
            | 
            | nlp = English()
            | doc = nlp(unprocessed_unicode)

mixin SeeAlso(name, link_target)
    a(href=link_target)
        span.declaration
            span.label via
            code= name


mixin Define(term)
  li
    #[span.declaration #[code #{term}]]
    block
  


mixin LexemeBooleans()
    ul
        +Define("is_alpha")
            |  Equivalent to #[code.language-python word.orth_.isalpha()]
        +Define("is_ascii")
            |  Equivalent to #[code.language-python any(ord(c) >= 128 for c in word.orth_)]
        +Define("is_digit")
            |  Equivalent to #[code.language-python word.orth_.isdigit()]
        +Define("is_lower")
            |  Equivalent to #[code.language-python word.orth_.islower()]
        +Define("is_title")
            |  Equivalent to #[code.language-python word.orth_.istitle()]
        +Define("is_punct")
            |  Equivalent to #[code.language-python word.orth_.ispunct()]
        +Define("is_space")
            |  Equivalent to #[code.language-python word.orth_.isspace()]
        +Define("like_url")
            |  Does the word resembles a URL?
        +Define("like_num")
            |  Does the word represent a number? e.g. “10.9”, “10”, “ten”, etc
        +Define("like_email")
            |  Does the word resemble an email?
        +Define("is_oov")
            |  Is the word out-of-vocabulary?
        +Define("is_stop")
            | Is the word part of a "stop list"? Stop lists are used to improve the quality of topic models, by filtering out common, domain-general words.

mixin LexemeStrings
    ul
        +Define("orth / orth_")
            | The form of the word with no string normalization or processing, as it appears in the string, without trailing whitespace.
    
        +Define("lower / lower_")
            | The form of the word, but forced to lower-case, i.e. #[code.language-python lower = word.orth_.lower()]
    
        +Define("shape / shape_")
            | A transform of the word's string, to show orthographic features. The characters a-z are mapped to x, A-Z is mapped to X, 0-9 is mapped to d. After these mappings, sequences of 4 or more of the same character are truncated to length 4.  Examples: C3Po --> XdXx, favorite --> xxxx, :) --> :) 
        +Define("prefix / prefix_")
            | A length-N substring from the start of the word.  Length may vary by language; currently for English n=1, i.e. #[code.language-python prefix = word.orth_[:1]]
    
        +Define("suffix / suffix_")
            | A length-N substring from the end of the word.  Length may vary by language; currently for English n=3, i.e. #[code.language-python suffix = word.orth_[-3:]]


mixin LexemeDistributional
    ul
      +Define("prob")
        |  The unigram log-probability of the word, estimated from counts from a large corpus, smoothed using Simple Good Turing estimation.
      +Define("cluster")
        |  The Brown cluster ID of the word. These are often useful features for linear models. If you’re using a non-linear model, particularly a neural net or random forest, consider using the real-valued word representation vector, in Token.repvec, instead.
      +Define("vector")
        |  A “word embedding” representation: a dense real-valued vector that supports similarity queries between words. By default, spaCy currently loads vectors produced by the Levy and Goldberg (2014) dependency-based word2vec model.


mixin Func(type1, type2)
    #{"λ " + type1 + ", " + type2}
 
// TODO
// Doc
// to_array
// count_by
// from_array
// from_bytes
// to_bytes
// read_bytes
// 
// Examples for repvec. Rename?

//- attr / parts_of_speech / entity_types / dependency_labels documentation
//- Gazetteer?

- var py_docs = '<a class="reference" href="http://docs.python.org/library/'

-
    var types = {
        'unicode': py_docs + 'functions.html#unicode"><em>unicode</em></a>',
        'bool': py_docs + 'functions.html#bool"><em>bool</em></a>',
        'int': py_docs + 'functions.html#int"><em>int</em></a>',
        'generator': "",
        'Vocab': "",
        'Span': "",
        'Doc': "",
        'StringStore': '#',
        'directory': '#'
    }

+declare_class("English", "pipeline")
    p Load models into a callable object to process English text.  Intended use is for one instance to be created per process.  You can create more if you're doing something unusual.   You may wish to make the instance a global variable or "singleton".  We usually instantiate the object in the #[code main()] function and pass it around as an explicit argument. 

    +summary
        +method("__init__", "self, data_dir=None, vocab=None, tokenizer=None, tagger=None, parser=None, entity=None, matcher=None, serializer=None)")(open=true)
            p Load the linguistic analysis pipeline.  Loading may take up to a minute, and the instance consumes 2 to 3 gigabytes of memory. The pipeline class is responsible for loading and saving the components, and applying them in sequence. Each component can be passed as an argument to the #[code __init__] function, or left as #[code None], in which case it will be loaded from a classmethod, named e.g. #[code default_vocab].
            
            p Common usage is to accept all defaults, in which case loading is simply:

            pre.language-python
                code
                    | >>> nlp = spacy.en.English()
                
            p To keep the default components, but load data from a specified directory, use:

            pre.language-python
                code
                    | >>> nlp = English(data_dir=u'path/to/data_directory')

            p To disable (and avoid loading) parts of the processing pipeline:

            pre.language-python
                code
                    | >>> nlp = English(parser=False, tagger=False, entity=False)
            
            +params
                +param("data_dir") 
                    | The data directory.  If #[code None], value is obtained via the #[code default_data_dir()] method.

                +param("vocab")
                    | The #[code vocab] object, which should be an instance of class #[code spacy.vocab.Vocab]. If #[code None], the object is obtained from the #[code default_vocab()] class method.  The #[code vocab] object manages all of the language specific rules and definitions, maintains the cache of lexical types, and manages the word vectors. Because the #[code vocab] owns this important data, most objects hold a reference to the #[code vocab].
                
                +param("tokenizer") 
                    | The tokenizer, which should be a callable that accepts a unicode string, and returns a #[code Doc] object. If set to #[code None], the default tokenizer is constructed from the #[code default_tokenizer()] method.
 
                +param("tagger") 
                    | The part-of-speech tagger, which should be a callable that accepts a #[code Doc] object, and sets the part-of-speech tags in-place. If set to #[code None], the default tagger is constructed from the #[code default_tagger()] method.
 
                +param("parser") 
                    | The dependency parser, which should be a callable that accepts a #[code Doc] object, and sets the syntactic heads and dependency labels in-place. If set to #[code None], the default parser is constructed from the #[code default_parser()] method.
  
                +param("entity") 
                    | The named entity recognizer, which should be a callable that accepts a #[code Doc] object, and sets the named entity annotations in-place. If set to #[code None], the default entity recognizer is constructed from the #[code default_entity()] method.
                   
                +param("matcher") 
                    | The pattern matcher, which should be a callable that accepts a #[code Doc] object, and sets annotations in-place. If set to #[code None], the default matcher is constructed from the #[code default_matcher()] method.
 
    +method("__call__", "text, tag=True, parse=True, entity=True", "English-__call__")(open="true")
        p The main entry point to spaCy. Takes raw unicode text, and returns a #[code Doc] object, which can be iterated to access #[code Token] and #[code Span] objects.  spaCy's models are all linear-time, so you can supply documents of arbitrary length, e.g. whole novels.

        +params
            +param("text", types.unicode)
                | The text to be processed.  spaCy expects raw unicode txt &ndash; you don't necessarily need to, say, split it into paragraphs.  However, depending on your documents, you might be better off applying custom pre-processing.  Non-text formatting, e.g. from HTML mark-up, should be removed before sending the document to spaCy.  If your documents have a consistent format, you may be able to improve accuracy by pre-processing.  For instance, if the first word of your documents are always in upper-case, it may be helpful to normalize them before supplying them to spaCy.

            +param("tag", types.bool)
                | Whether to apply the part-of-speech tagger. Required for parsing and entity recognition.

            +param("parse", types.bool)
                |  Whether to apply the syntactic dependency parser.

            +param("entity", types.bool)
                | Whether to apply the named entity recognizer.

        pre.language-python
            code
                | from spacy.en import English
                | nlp = English()
                | doc = nlp(u'Some text.) # Applies tagger, parser, entity
                | doc = nlp(u'Some text.', parse=False) # Applies tagger and entity, not parser
                | doc = nlp(u'Some text.', entity=False) # Applies tagger and parser, not entity
                | doc = nlp(u'Some text.', tag=False) # Does not apply tagger, entity or parser
                | doc = nlp(u'') # Zero-length tokens, not an error
                | # doc = nlp(b'Some text') <-- Error: need unicode
                | doc = nlp(b'Some text'.decode('utf8')) # Encode to unicode first.


+declare_class("Doc", "doc")
    p A sequence of #[code Token] objects.  Access sentences and named entities, export annotations to numpy arrays, losslessly serialize to compressed binary strings.

    p Internally, the #[code Doc] object holds an array of #[code TokenC] structs.  The Python-level #[code Token] and #[code Span] objects are views of this array, i.e. they don't own the data themselves. This details of the internals shouldn't matter for the API &ndash; but it may help you read the code, and understand how spaCy is designed.

    +init
        +SeeAlso("English.__call__(unicode text)", "#English-__call__")
                
        +method("__init__", "self, vocab, orth_and_spaces=None")(open=true)
            |  This method of constructing a #[code Doc] object is usually only used for deserialization.  Standard usage is to construct the document via a call to the language object.
            +params
                +param("vocab", vocab_type)
                    |  A Vocabulary object, which must match any models you want to use (e.g. tokenizer, parser, entity recognizer).
                +param("orth_and_spaces")
                    |  A list of #[code (orth_id, has_space)] tuples, where #[code orth_id] is an integer, and has_space is a boolean, indicating whether the token has a trailing space.

    +sequence
        ul
            +Define("doc[i]")
                |  Get the #[code Token] object at position #[code i], where #[code i] is an integer. Negative indexing is supported, and follows the usual Python semantics, i.e. #[code doc[-2]] is <code>doc[len(doc) - 2]</code>.

            +Define("doc[start : end]")
                |  Get a #[code Span] object, starting at position #[code start] and ending at position #[code end]. For instance, <code>doc[2:5]</code> produces a span consisting of tokens 2, 3 and 4. Stepped slices (e.g. <code>doc[start : end : step]</code>) are not supported, as #[code Span] objects must be contiguous (cannot have gaps).

            +Define("for token in doc")
                | Iterate over #[code Token ] objects, from which the annotations can be easily accessed.  This is  the main way of accessing #[code Token] objects, which are the main way annotations are accessed from Python. If faster-than-Python speeds are required, you can instead access the annotations as a numpy array, or access the underlying C data directly from Cython, via #[code Doc.data], an array of #[code TokenC] structs. The C API has not yet been finalized, and is subject to change.

            +Define("len(doc)")
                |  The number of tokens in the document.

    details
        summary: h4 Sentence, entity and noun chunk spans
        
        +attribute("sents", types.generator)(open=true)
            |  Yields sentence #[code Span] objects.  Iterate over the span to get individual #[code Token] objects.  Sentence spans have no label.
            pre.language-python
                code
                    | >>> from spacy.en import English
                    | >>> nlp = English()
                    | >>> doc = nlp(u'This is a sentence. Here's another...')
                    | >>> for sentence in doc.sents:
                    | ...     sentence.root.orth_
                    | is
                    | 's
 
    
        +attribute("ents", types.generator)(open=true)
            |  Yields named-entity #[code Span] objects.  Iterate over the span to get individual #[code Token] objects, or access the label:
            pre.language-python
                code
                    | >>> from spacy.en import English
                    | >>> nlp = English()
                    | >>> tokens = nlp(u'Mr. Best flew to New York on Saturday morning.')
                    | >>> ents = list(tokens.ents)
                    | >>> ents[0].label, ents[0].label_, ents[0].orth_, ents[0].string
                    | (112504, 'PERSON', 'Best', ents[0].string) 
 
        +attribute("noun_chunks", types.generator)(open=true)
            |  Yields base noun-phrase #[code Span ] objects.  A base noun phrase, or "NP chunk", is a noun phrase that does not permit other NPs to be nested within it &ndash; so no NP-level coordination, no prepositional phrases, and no relative clauses.  For example:
            pre.language-python
                code
                    | >>> from spacy.en import English
                    | >>> nlp = English()
                    | >>> doc = nlp('The sentence in this example has three noun chunks.')
                    | >>> for chunk in doc.noun_chunks:
                    | ...     print(chunk.label, chunk.orth_, '<--', chunk.root.head.orth_)
                    | NP The sentence <-- has
                    | NP this example <-- in
                    | NP three noun chunks <-- has
    
    details
        summary: h4 Export/Import
        
        +method("to_array", "attr_ids")(open=true)
            | Given a list of M attribute IDs, export the tokens to a numpy ndarray of shape N*M, where N is the length of the sentence.

            +params
                +param("attr_ids", "list[int]")(open=true)
                    | A list of attribute ID ints.  Attribute IDs can be imported from  
                    code spacy.attrs

        +method("count_by", "attr_id")(open=true)
            | Produce a dict of #[code {attribute (int): count (ints)}] frequencies, keyed by the values of the given attribute ID.
        
            pre.language-python
                code
                    | >>> from spacy.en import English, attrs
                    | >>> nlp = English()
                    | >>> tokens = nlp(u'apple apple orange banana')
                    | >>> tokens.count_by(attrs.ORTH)
                    | {12800L: 1, 11880L: 2, 7561L: 1}
                    | >>> tokens.to_array([attrs.ORTH])
                    | array([[11880],
                    |         [11880],
                    |         [7561],
                    |         [12800]])

        +method("from_array", "attrs, array")(open=true)
            Write to a #[code Doc] object, from an M*N array of attributes.
    
        +method("from_bytes", "byte_string")(open=true)
            | Deserialize, loading from bytes.

        +method("to_bytes")(open=true)
            | Serialize, producing a byte string.

        +method("read_bytes")(open=true)
            | A staticmethod, used to read serialized #[code Doc] objects from a file.
            | For example:
            pre.language-python
                code
                    | for byte_string in Doc.read_bytes(open(location_of_bytes)):
                    |     doc = Doc(nlp.vocab).from_bytes(byte_string)

+declare_class("Token", "token")
    p A Token represents a single word, punctuation or significant whitespace symbol. Integer IDs are provided for all string features. The (unicode) string is provided by an attribute of the same name followed by an underscore, e.g. #[code token.orth] is an integer ID, #[code token.orth_] is the unicode value. The only exception is the Token.string attribute, which is (unicode) string-typed.
    details
        summary: h4 String Features
  
        ul
            +Define("lemma / lemma_")
                | The "base" of the word, with no inflectional suffixes, e.g. the lemma of "developing" is "develop", the lemma of "geese" is "goose", etc.  Note that #[em derivational] suffixes are not stripped, e.g. the lemma of "instutitions" is "institution", not "institute".  Lemmatization is performed using the WordNet data, but extended to also cover closed-class words such as pronouns.  By default, the WN lemmatizer returns "hi" as the lemma of "his". We assign pronouns the lemma #[code -PRON-].  

        +LexemeStrings()

    details
        summary: h4 Boolean Flags
        +LexemeBooleans

        +method("check_flag", "flag_id")(open=true)
            | Get the value of one of the boolean flags

    details
        summary: h4 Distributional Features
        +LexemeDistributional()

    details
        summary: h4 Alignment and Output

        ul
            +Define("idx")
                | Start index of the token in the string

            +Define("len(token)", "")
                | Length of the token's orth string, in unicode code-points.

            +Define("unicode(token)", "")
                | Same as #[code token.orth_]

            +Define("str(token)", "")
                | In Python 3, returns #[code token.orth_]. In Python 2, returns
                | #[code token.orth_.encode('utf8')]

            +Define("text")
                | An alias for #[code token.orth_].

            +Define("text_with_ws")
                | #[code token.orth_ + token.whitespace_], i.e. the form of the word as it appears in the string, #[including trailing whitespace].  This is useful when you need to use linguistic features to add inline mark-up to the string.

            +Define("whitespace_")
                | The number of immediate syntactic children following the word in the string.

        define
            summary: h4 Navigating the Parse Tree
    
            ul
                +Define("head")
                    | The immediate syntactic head of the token.  If the token is the root of its sentence, it is the token itself, i.e. #[code root_token.head is root_token]

                +Define("children")
                    | An iterator that yields from lefts, and then yields from rights.
    
                +Define("subtree")
                    | An iterator for the part of the sentence syntactically governed by the word, including the word itself.
    
                +Define("left_edge")
                    | The leftmost edge of the token's subtree
    
                +Define("right_edge")
                    | The rightmost edge of the token's subtree

            +method("nbor(i=1)")(open=true)
                | Get the #[em i]th next / previous neighboring token.

    details
        summary: h4 Named Entities

        ul
            +Define("ent_type")
                | If the token is part of an entity, its entity type.

            +Define("ent_iob")
                | The IOB (inside, outside, begin) entity recognition tag for the token.

    +init
        +method("__init__", "vocab, doc, offset")(open=true)
            +params
                +param("vocab", types.Vocab)
                    | A Vocab object

                +param("doc", types.Doc)
                    | The parent sequence

                +param("offset", types.int)
                    | The index of the token within the document

        //+attribute("conjuncts")
        //  | Conjuncts

+declare_class("Span", "span")
    | A #[code Span] is a slice of a #[code Doc] object, consisting of zero or more tokens.  Spans are used to represent sentences, named entities, phrases, and arbitrary contiguous slices from the #[code Doc] object.  #[code Span] objects are views &ndash; that is, they do not copy the underlying C data.  This makes them cheap to construct, as internally are simply a reference to the #[code Doc] object, a start position, an end position, and a label ID.

    +Define("token = span[i]")
        | Get the #[code Token] object at position #[em i], where #[em i] is an offset within the #[code Span], not the document.  That is:
        pre.language-python
            code
                | span = doc[4:6]
                | token = span[0]
                | assert token.i == 4

    ul
        +Define("for token in span")
            | Iterate over the #[code Token] objects in the span.
                    
        +Define("__len__")
            | Number of tokens in the span.

        +Define("text")
            | The text content of the span, obtained from #[code.language-python ''.join(token.text_with_ws for token in span)]

        +Define("start")
            | The start offset of the span, i.e. #[code.language-python span[0].i].

        +Define("end")
            | The end offset of the span, i.e. #[code.language-python span[-1].i + 1]

    details
        summary: h4 Navigating the Parse Tree

        +attribute("root")(open=true)
            | The first ancestor of the first word of the span that has its head outside the span. For example:
            pre.language-python
                code
                    | >>> toks = nlp(u'I like New York in Autumn.')

            p Let's name the indices --- easier than writing #[code toks[4]] etc.

            pre.language-python
                code
                    | >>> i, like, new, york, in_, autumn, dot = range(len(toks)) 

            p The head of #[em new] is #[em York], and the head of #[em York] is #[em like]
            pre.language-python
                code
                    | >>> toks[new].head.orth_
                    | 'York'
                    | >>> toks[york].head.orth_
                    | 'like'

            p Create a span for "New York". Its root is "York".
            pre.language-python
                code
                    | >>> new_york = toks[new:york+1]
                    | >>> new_york.root.orth_
                    | 'York'

            p When there are multiple words with external dependencies, we take the first:

            pre.language-python
                code
                    | >>> toks[autumn].head.orth_, toks[dot].head.orth_
                    | ('in', like')
                    | >>> autumn_dot = toks[autumn:]
                    | >>> autumn_dot.root.orth_
                    | 'Autumn'

        +attribute("lefts")(open=true)
            | Tokens that are to the left of the span, whose head is within the span, i.e. 
            code.language-python
                | lefts = [span.doc[i] for i in range(0, span.start)
                |          if span.doc[i].head in span]

        +attribute("rights")(open=true)
            | Tokens that are to the right of the span, whose head is within the span, i.e.
            code.language-python
                | rights = [span.doc[i] for i in range(span.end, len(span.doc))
                |           if span.doc[i].head in span]


        +attribute("subtree")(open=true)
            | Tokens in the range #[code (start, end+1)], where #[code start] is the index of the leftmost word descended from a token in the span, and #[code end] is the index of the rightmost token descended from a token in the span.

    +init
        ul
            +Define("doc[start : end]")
            +Define("for entity in doc.ents")
            +Define("for sentence in doc.sents")
            +Define("for noun_phrase in doc.noun_chunks")
            +Define("span = Span(doc, start, end, label=0)")

    details
        summary: h4 Strings

        ul
            +Define("text_with_ws")
                | The form of the span as it appears in the string, #[including trailing whitespace].  This is useful when you need to use linguistic features to add inline mark-up to the string.

            +Define("lemma / lemma_")
                | Whitespace-concatenated lemmas of each token in the span.

            +Define("label / label_")
                | The span label, used particularly for named entities.

+declare_class("Lexeme", "lexeme")
    p The Lexeme object represents a lexical type, stored in the vocabulary &ndash; as opposed to a token, occurring in a document.

    p Each #[code Token] object receives a reference to a lexeme object (specifically, it receives a pointer to a #[code LexemeC] struct). This allows features to be computed and saved once per #[em type], rather than once per #[em token]. As job sizes grow, this amounts to substantial efficiency improvements, as the vocabulary size (number of types) will be much smaller than the total number of words processed (number of tokens).

    p All Lexeme attributes are therefore context independent, as a single lexeme is reused for all usages of that word. Lexemes are keyed by the “orth” attribute.  

    p Most Lexeme attributes can be set, with the exception of the primary key, #[code orth]. Assigning to an attribute of the Lexeme object writes to the underlying struct, so all tokens that are backed by that Lexeme will inherit the new value.

    details
        summary: h4 String Features
        +LexemeStrings
    
    details
        summary: h4 Boolean Features
        +LexemeBooleans

    details
        summary: h4 Distributional Features
        +LexemeDistributional

    +init(open=true)
        ul
            +Define("lexeme = vocab[string]")
            +Define("lexeme = vocab[i]")



+declare_class("Vocab")
    ul
        +Define("lexeme = vocab[integer_id]")(open="true")
            | Get a lexeme by its orth ID
    
        +Define("lexeme = vocab[string]")(open="true")
            | Get a lexeme by the string corresponding to its orth ID.
    
        +Define("for lexeme in vocab")
            | Iterate over #[code Lexeme] objects
    
    
        +Define("vocab[integer_id] = attributes_dict")(open=true)
            | A props dictionary
     
        +Define("len(vocab)")(open=true)
            | Number of lexemes (unique words) in the 

    +init(open=true)
        ul
            +Define("nlp.vocab")
            +Define("doc.vocab")
            +Define("span.vocab")
            +Define("token.vocab")
            +Define("lexeme.vocab")

    details
        summary: h4 Save and Load

        +method("dump", "loc")(open=true)
            +params
                +param("loc", types.unicode)
                    | Path where the vocabulary should be saved

        +method("load_lexemes", "loc")(open=true)
            +params
                +param("loc", types.unicode)
                    | Path to load the lexemes.bin file from

        +method("load_vectors", "file")(open=true)
            +params
                +param("file", types.unicode)
                    | A file-like object, to load word vectors from.
        
        +method("load_vectors_from_bin_loc", "loc")(open=true)
            +params
                +param("loc", types.unicode)
                    | A path to a file, in spaCy's binary word-vectors file format.


+declare_class("StringStore")
    p Intern strings, and map them to sequential integer IDs. The mapping table is very efficient , and a small-string optimization is used to maintain a small memory footprint.  Only the integer IDs are held by spaCy's data classes (#[code Doc], #[code Token], #[code Span] and #[code Lexeme]) &ndash; when you use a string-valued attribute like #[code token.orth_], you access a property that computes #[code token.strings[token.orth]].
    ul
        +Define("string = string_store[int_id]")
            | Retrieve a string from a given integer ID. If the integer ID is not found, raise #[code IndexError]

        +Define("int_id = string_store[unicode_string]")
            |  Map a unicode string to an integer ID. If the string is previously unseen, it is interned, and a new ID is returned. 

        +Define("int_id = string_store[utf8_byte_string]")
            |  Byte strings are assumed to be in UTF-8 encoding.  Strings encoded with other codecs may fail silently.  Given a utf8 string, the behaviour is the same as for unicode strings.  Internally, strings are stored in UTF-8 format.  So if you start with a UTF-8 byte string, it's less efficient to first decode it as unicode, as StringStore will then have to encode it as UTF-8 once again.  
        +Define("n_strings = len(string_store)")
            | Number of strings in the string-store

        +Define("for string in string_store")(open=true)
            | Iterate over strings in the string store, in order, such that the #[em i]th string in the sequence has the ID #[em i]:
            pre.language-python
                code
                    | for i, string in enumerate(string_store):
                    |     assert i == string_store[string]

    +init
        p #[code StringStore.__init__] takes no arguments, so a new instance can be constructed as follows:
        pre.language-python
            code
                | string_store = StringStore()

        p However, in practice you'll usually use the instance owned by the language's #[code vocab] object, which all classes hold a reference to:

        ul
            li #[code.language-python english.vocab.strings]
            li #[code.language-python doc.vocab.strings]
            li #[code.language-python span.vocab.strings]
            li #[code.language-python token.vocab.strings]
            li #[code.language-python lexeme.vocab.strings]

        p If you create another instance, it will map strings to different integers &ndash; which is usually not what you want.

    details
        summary: h4 Save and Load

        +method("dump", "loc")(open=true)
            p Save the strings mapping to the given location, in plain text. The format is subject to change; so if you need to read/write compatible files, please can find details in the #[code strings.pyx] source.

        +method("load", "loc")(open=true)
            p Load the strings mapping from a plain-text file in the given location.  The format is subject to change; so if you need to read/write compatible files, please can find details in the #[code strings.pyx] source.
